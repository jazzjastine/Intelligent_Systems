{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd75073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b481af9-3e7a-4407-8bde-8d01b5d3b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56591d37-365b-4b44-b0a2-f00900104823",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f3c3a-5ca9-4272-8d27-c5af9b0754c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Artificial intelligence is reshaping industries through innovative automation.\",\n",
    "    \"Robotics and AI applications revolutionize manufacturing processes.\",\n",
    "    \"Ethical considerations in AI development are crucial for responsible innovation.\",\n",
    "    \"AI-driven advancements pose challenges in workforce adaptation and job displacement.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04ad12-95ea-4a0a-ac49-f4ee9fbaeee2",
   "metadata": {},
   "source": [
    "# Preprocess the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f87a2-a9f4-4557-aef6-c921bda4e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize and lemmatize using spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all documents\n",
    "processed_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Create a dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(processed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model1 = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "lda_model2 = gensim.models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05542fa2-2765-4327-bb81-27e6b2d629ef",
   "metadata": {},
   "source": [
    "# Print topics and their keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f567bec-dfdf-4dd8-80b3-169eb11aa978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.234*\"learning\" + 0.121*\"machine\" + 0.084*\"type\" + 0.047*\"network\" + '\n",
      "  '0.047*\"artificial\" + 0.047*\"base\" + 0.047*\"subset\" + 0.047*\"deep\" + '\n",
      "  '0.047*\"neural\" + 0.047*\"supervise\"'),\n",
      " (1,\n",
      "  '0.053*\"machine\" + 0.053*\"learning\" + 0.053*\"reinforcement\" + '\n",
      "  '0.053*\"technique\" + 0.053*\"main\" + 0.053*\"unsupervised\" + 0.053*\"supervise\" '\n",
      "  '+ 0.053*\"type\" + 0.053*\"neural\" + 0.053*\"subset\"'),\n",
      " (2,\n",
      "  '0.053*\"machine\" + 0.053*\"learning\" + 0.053*\"reinforcement\" + '\n",
      "  '0.053*\"technique\" + 0.053*\"supervise\" + 0.053*\"main\" + 0.053*\"unsupervised\" '\n",
      "  '+ 0.053*\"type\" + 0.053*\"neural\" + 0.053*\"deep\"'),\n",
      " (3,\n",
      "  '0.106*\"machine\" + 0.106*\"experience\" + 0.106*\"automatically\" + '\n",
      "  '0.106*\"improve\" + 0.106*\"involve\" + 0.106*\"algorithm\" + 0.106*\"learning\" + '\n",
      "  '0.021*\"reinforcement\" + 0.021*\"technique\" + 0.021*\"unsupervised\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model2.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000de19a-8dd4-40e5-966b-25442d67ce78",
   "metadata": {},
   "source": [
    "# Assign topics to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da343547-03b4-41b6-b1b6-1faba58a87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Topic: [(0, 0.032817908), (1, 0.03134291), (2, 0.031342912), (3, 0.90449625)]\n",
      "Document 2 - Topic: [(0, 0.924469), (1, 0.025126709), (2, 0.02512671), (3, 0.025277598)]\n",
      "Document 3 - Topic: [(0, 0.89203846), (1, 0.035840042), (2, 0.035840042), (3, 0.036281466)]\n",
      "Document 4 - Topic: [(0, 0.9055772), (1, 0.03137502), (2, 0.03137502), (3, 0.031672765)]\n"
     ]
    }
   ],
   "source": [
    "# Assign topics to documents\n",
    "for i, doc in enumerate(processed_documents):\n",
    "    print(f\"Document {i+1} - Topic: {lda_model1.get_document_topics(corpus[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(processed_documents):\n",
    "    print(f\"Document {i+1} - Topic: {lda_model2.get_document_topics(corpus[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17631d05-4439-4d2d-a59a-fe43b98bf5f9",
   "metadata": {},
   "source": [
    "#                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
