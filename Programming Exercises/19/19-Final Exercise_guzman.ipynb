{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZcv4vg7Tr0U"
      },
      "source": [
        "## Final Exercise\n",
        "\n",
        "Paper source: https://arxiv.org/pdf/1611.03530.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGRlKLMcTr0e"
      },
      "source": [
        "## Imports and initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "awTg-UgMTr0e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "8IO35089Tr0h"
      },
      "outputs": [],
      "source": [
        "input_channels = 3\n",
        "num_classes = 10\n",
        "\n",
        "device = 'cuda'\n",
        "lr = 0.01\n",
        "epochs = 64\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlM8roZgTr0i"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "yMORsbErTr0j"
      },
      "outputs": [],
      "source": [
        "# create a list of commands for transforming the image\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # convert image to tensor\n",
        "    transforms.CenterCrop(28) # crop from the center\n",
        "])\n",
        "\n",
        "# function to normalize each image\n",
        "def norm_image(data_sample):\n",
        "    img_tensor = data_sample[0] # image tensors\n",
        "    label = data_sample[1]\n",
        "\n",
        "    img_means = img_tensor.mean(axis=[1,2]) # mean of image per channel\n",
        "    img_sds = img_tensor.std(axis=[1,2]) # overall SD of image per channel\n",
        "\n",
        "    mean_sub = img_tensor - img_means.unsqueeze(1).unsqueeze(2)\n",
        "    img_norm = mean_sub.true_divide(img_sds.unsqueeze(1).unsqueeze(2))\n",
        "\n",
        "    return (img_norm, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzvvEA-nTr0j",
        "outputId": "db95d874-a59d-4def-c41b-805bbaa773f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# training set\n",
        "\n",
        "all_train = list(datasets.CIFAR10(root = 'data/', transform=img_transform, train = True, download=True))\n",
        "\n",
        "random.shuffle(all_train)\n",
        "\n",
        "train_data = all_train[:40000]\n",
        "train_transformed = list(map(norm_image, train_data))\n",
        "train_loader = DataLoader(dataset=train_transformed, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_data = all_train[40000:]\n",
        "val_transformed = list(map(norm_image, val_data))\n",
        "val_loader = DataLoader(dataset=val_transformed, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = datasets.CIFAR10(root='data/', transform=img_transform, train=False, download=True)\n",
        "test_transformed = list(map(norm_image, val_data))\n",
        "test_loader = DataLoader(dataset=val_transformed, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzV2W-sATr0v"
      },
      "source": [
        "## Implementation of Reduced GoogLeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "ffb2hjuATr0w"
      },
      "outputs": [],
      "source": [
        "# convolution module\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.relu = nn.ReLU() # ReLU\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs) # convolution\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) # batch normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "# inception module\n",
        "class inception_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_ch1, out_ch3):\n",
        "        super(inception_block, self).__init__()\n",
        "        # first conv module, same padding\n",
        "        self.ch1 = conv_block(in_channels=in_channels, out_channels=out_ch1, kernel_size=(3,3), stride=(1,1), padding='same')\n",
        "        # second conv module, same padding\n",
        "        self.ch3 = conv_block(in_channels=in_channels, out_channels=out_ch3, kernel_size=(3,3), stride=(1,1), padding='same')\n",
        "\n",
        "    def forward(self,x):\n",
        "        # concatenate convolution outputs\n",
        "        return torch.cat([self.ch1(x),self.ch3(x)],1)\n",
        "\n",
        "# downsample module\n",
        "class downsample_block(nn.Module):\n",
        "    def __init__(self, in_channels, conv_out):\n",
        "        super(downsample_block, self).__init__()\n",
        "        # conv module\n",
        "        self.convblock = conv_block(in_channels, conv_out, kernel_size=(3,3), stride=(2,2))\n",
        "        # max pooling\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return torch.cat([self.convblock(x),self.maxpool(x)],1)\n",
        "\n",
        "# the mini GoogLeNet\n",
        "class mini_GoogLeNet(nn.Module):\n",
        "    def __init__(self,in_channels=3, num_classes=10, dropout_prob=0):\n",
        "        super(mini_GoogLeNet, self).__init__()\n",
        "        self.conv1 = conv_block(in_channels=3, out_channels=64, kernel_size=(3,3), stride=(1,1))\n",
        "        self.inception1 = inception_block(64,32,32)\n",
        "        self.inception2 = inception_block(64,32,48)\n",
        "        self.downsample1 = downsample_block(80,80)\n",
        "        self.inception3 = inception_block(160,96,64)\n",
        "        self.inception4 = inception_block(160,96,64)\n",
        "        self.inception5 = inception_block(160,80,80)\n",
        "        self.inception6 = inception_block(160,80,64)\n",
        "        self.downsample2 = downsample_block(144,96)\n",
        "        self.inception7 = inception_block(240,176,160)\n",
        "        self.inception8 = inception_block(336,176,160)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=(7,7), padding=(1,1))\n",
        "        self.dropout = nn.Dropout(p = dropout_prob)\n",
        "        self.fc = nn.Linear(336,10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = self.downsample1(x)\n",
        "        x = self.inception3(x)\n",
        "        x = self.inception4(x)\n",
        "        x = self.inception5(x)\n",
        "        x = self.inception6(x)\n",
        "        x = self.downsample2(x)\n",
        "        x = self.inception7(x)\n",
        "        x = self.inception8(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0],-1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46dzdgGcTr0w"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "FFbQC9KVTr0x"
      },
      "outputs": [],
      "source": [
        "# create the model\n",
        "model = mini_GoogLeNet(in_channels=3, num_classes=10, dropout_prob=0).to(device=device)\n",
        "\n",
        "# parameter settings\n",
        "loss_function = nn.CrossEntropyLoss() # loss funtion\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr) # optimizer\n",
        "scheduler = optim.lr_scheduler.LinearLR(optimizer) # scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "62yGbK5RTr0x"
      },
      "outputs": [],
      "source": [
        "# initialize results\n",
        "train_acc = []\n",
        "train_all_loss = []\n",
        "\n",
        "test_acc = []\n",
        "test_all_loss = []\n",
        "\n",
        "con_mats = []\n",
        "\n",
        "times = []\n",
        "train_times = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "Zc9jlRd2Tr0x"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    curr_loss_train = 0 # loss\n",
        "    correct_train = 0 # correctly classified training points\n",
        "    total_train = 0 # total number of training points\n",
        "\n",
        "    for ind, (data_train, true_labels_train) in enumerate(train_loader):\n",
        "        data_train = data_train.to(device=device) # use GPU\n",
        "        true_labels_train = true_labels_train.to(device=device) # use GPU\n",
        "\n",
        "        out_train = model(data_train) # apply model to training data\n",
        "        loss_train = loss_function(out_train, true_labels_train) # get loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        curr_loss_train += loss_train.item()\n",
        "        ix, predicted_train = out_train.max(1)\n",
        "        correct_train += predicted_train.eq(true_labels_train).sum().item()\n",
        "        total_train += true_labels_train.size(0)\n",
        "\n",
        "    train_loss = curr_loss_train/len(train_loader) # get loss\n",
        "    acc_train_val = (correct_train/total_train)*100 # get accuracy\n",
        "\n",
        "    train_acc.append(acc_train_val)\n",
        "    train_all_loss.append(train_loss)\n",
        "\n",
        "# testing\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    curr_loss_test = 0 # loss\n",
        "    correct_test = 0 # correctly classified test points\n",
        "    total_test = 0 # total number of test points\n",
        "\n",
        "    num_class = 10\n",
        "    confusion_matrix = torch.zeros(num_class, num_class)\n",
        "    with torch.no_grad():\n",
        "        for data_test, true_labels_test in test_loader:\n",
        "\n",
        "            data_test = data_test.to(device=device) # use GPU\n",
        "            true_labels_test = true_labels_test.to(device=device) # use GPU\n",
        "\n",
        "            out_test = model(data_test) # apply model to training data\n",
        "            loss_test = loss_function(out_test, true_labels_test) # get loss\n",
        "\n",
        "            # metrics\n",
        "            curr_loss_test += loss_test.item()\n",
        "            ix, predicted_test = out_test.max(1)\n",
        "            correct_test += predicted_test.eq(true_labels_test).sum().item()\n",
        "            total_test += true_labels_test.size(0)\n",
        "\n",
        "\n",
        "    test_loss = curr_loss_test/len(test_loader) # get loss\n",
        "    acc_test_val = (correct_test/total_test)*100 # get accuracy\n",
        "\n",
        "    test_acc.append(acc_test_val)\n",
        "    test_all_loss.append(test_loss)\n",
        "    con_mats.append(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjF4y3LATr0y",
        "outputId": "152fec3e-7247-4f99-8e09-57b17e8cebe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Epoch time: 22.66 seconds\n",
            "Epoch 1\n",
            "Epoch time: 22.81 seconds\n",
            "Epoch 2\n",
            "Epoch time: 23.04 seconds\n",
            "Epoch 3\n",
            "Epoch time: 23.24 seconds\n",
            "Epoch 4\n",
            "Epoch time: 23.41 seconds\n",
            "Epoch 5\n",
            "Epoch time: 23.56 seconds\n",
            "Epoch 6\n",
            "Epoch time: 23.65 seconds\n",
            "Epoch 7\n",
            "Epoch time: 23.77 seconds\n",
            "Epoch 8\n",
            "Epoch time: 23.86 seconds\n",
            "Epoch 9\n",
            "Epoch time: 23.93 seconds\n",
            "Epoch 10\n",
            "Epoch time: 23.97 seconds\n",
            "Epoch 11\n",
            "Epoch time: 24.01 seconds\n",
            "Epoch 12\n",
            "Epoch time: 24.07 seconds\n",
            "Epoch 13\n",
            "Epoch time: 24.16 seconds\n",
            "Epoch 14\n",
            "Epoch time: 24.22 seconds\n",
            "Epoch 15\n",
            "Epoch time: 24.14 seconds\n",
            "Epoch 16\n",
            "Epoch time: 24.14 seconds\n",
            "Epoch 17\n",
            "Epoch time: 24.19 seconds\n",
            "Epoch 18\n",
            "Epoch time: 24.13 seconds\n",
            "Epoch 19\n",
            "Epoch time: 24.15 seconds\n",
            "Epoch 20\n",
            "Epoch time: 24.19 seconds\n",
            "Epoch 21\n",
            "Epoch time: 24.19 seconds\n",
            "Epoch 22\n",
            "Epoch time: 24.17 seconds\n",
            "Epoch 23\n",
            "Epoch time: 24.15 seconds\n",
            "Epoch 24\n",
            "Epoch time: 24.14 seconds\n",
            "Epoch 25\n",
            "Epoch time: 24.23 seconds\n",
            "Epoch 26\n",
            "Epoch time: 24.15 seconds\n",
            "Epoch 27\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 28\n",
            "Epoch time: 24.09 seconds\n",
            "Epoch 29\n",
            "Epoch time: 24.14 seconds\n",
            "Epoch 30\n",
            "Epoch time: 24.16 seconds\n",
            "Epoch 31\n",
            "Epoch time: 24.13 seconds\n",
            "Epoch 32\n",
            "Epoch time: 24.15 seconds\n",
            "Epoch 33\n",
            "Epoch time: 24.12 seconds\n",
            "Epoch 34\n",
            "Epoch time: 24.17 seconds\n",
            "Epoch 35\n",
            "Epoch time: 24.16 seconds\n",
            "Epoch 36\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 37\n",
            "Epoch time: 24.13 seconds\n",
            "Epoch 38\n",
            "Epoch time: 24.09 seconds\n",
            "Epoch 39\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 40\n",
            "Epoch time: 24.10 seconds\n",
            "Epoch 41\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 42\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 43\n",
            "Epoch time: 24.12 seconds\n",
            "Epoch 44\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 45\n",
            "Epoch time: 24.10 seconds\n",
            "Epoch 46\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 47\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 48\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 49\n",
            "Epoch time: 24.12 seconds\n",
            "Epoch 50\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 51\n",
            "Epoch time: 24.10 seconds\n",
            "Epoch 52\n",
            "Epoch time: 24.10 seconds\n",
            "Epoch 53\n",
            "Epoch time: 24.08 seconds\n",
            "Epoch 54\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 55\n",
            "Epoch time: 24.09 seconds\n",
            "Epoch 56\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 57\n",
            "Epoch time: 24.10 seconds\n",
            "Epoch 58\n",
            "Epoch time: 24.11 seconds\n",
            "Epoch 59\n",
            "Epoch time: 24.12 seconds\n",
            "Epoch 60\n",
            "Epoch time: 24.10 seconds\n",
            "Epoch 61\n",
            "Epoch time: 24.12 seconds\n",
            "Epoch 62\n",
            "Epoch time: 24.09 seconds\n",
            "Epoch 63\n",
            "Epoch time: 24.12 seconds\n",
            "\n",
            "Total training time: 1536.59 seconds\n"
          ]
        }
      ],
      "source": [
        "train_start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    ep_start = time.time()\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_time = time.time() - ep_start\n",
        "    print(f\"Epoch time: {epoch_time:0.2f} seconds\")\n",
        "    times.append(epoch_time)\n",
        "\n",
        "    train_time = time.time() - train_start\n",
        "    train_times.append(train_time)\n",
        "\n",
        "print()\n",
        "print(f\"Total training time: {train_time:0.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e0AQ1ahTr0y"
      },
      "source": [
        "## Export results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "aPbniu4MLeBs",
        "outputId": "296f6087-cd0e-42b2-c9e0-091f8650629c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "owKPEEyrTr0y"
      },
      "outputs": [],
      "source": [
        "# save all results to a dataframe for export\n",
        "df_res = pd.DataFrame()\n",
        "df_res['TrainAccuracy'] = train_acc\n",
        "df_res['TrainLoss'] = train_all_loss\n",
        "df_res['TestAccuracy'] = test_acc\n",
        "df_res['TestLoss'] = test_all_loss\n",
        "df_res['EpochTime'] = times\n",
        "df_res['TotalTime'] = train_times\n",
        "\n",
        "df_res.to_csv('/content/drive/My Drive/19-results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "u5vL6_pYTr0z"
      },
      "outputs": [],
      "source": [
        "# export all confusion matrices to a pickle\n",
        "with open('/content/drive/My Drive/19-results.pickle','wb') as handle:\n",
        "    pickle.dump(con_mats, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIID2g15Tr0z"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SPcL-nk6Tr0z",
        "outputId": "018f6987-7918-4f0b-8d89-87e87813508f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    TrainAccuracy  TrainLoss  TestAccuracy  TestLoss  EpochTime    TotalTime\n",
              "0         33.3675   1.971517         45.23  1.654450  22.663356    22.664520\n",
              "1         50.5450   1.471566         56.42  1.262992  22.812373    45.477028\n",
              "2         60.0850   1.178294         61.05  1.126179  23.044251    68.522231\n",
              "3         66.4175   0.983574         60.16  1.166780  23.236218    91.758553\n",
              "4         71.9750   0.828723         66.13  0.963742  23.405690   115.164344\n",
              "5         76.0225   0.712667         64.13  1.019803  23.564798   138.729245\n",
              "6         79.8350   0.603375         73.02  0.800099  23.654828   162.384912\n",
              "7         83.0150   0.507814         69.01  0.934989  23.766843   186.151879\n",
              "8         86.2400   0.421344         72.52  0.869184  23.856837   210.008825\n",
              "9         89.3150   0.336884         68.75  0.975435  23.928858   233.937788\n",
              "10        91.0975   0.280199         68.43  1.088275  23.969904   257.908579\n",
              "11        94.0325   0.204458         67.05  1.180282  24.005055   281.913751\n",
              "12        95.5450   0.159442         75.66  0.790307  24.066745   305.980646\n",
              "13        97.2275   0.109291         75.59  0.786821  24.164718   330.145481\n",
              "14        98.3575   0.073589         76.73  0.743234  24.215126   354.360719\n",
              "15        99.2700   0.045664         78.56  0.716787  24.144737   378.505563\n",
              "16        99.7750   0.024329         79.35  0.697416  24.141706   402.647385\n",
              "17        99.9125   0.015662         81.09  0.616303  24.190419   426.837930\n",
              "18        99.9550   0.010545         80.91  0.630850  24.131956   450.969998\n",
              "19        99.9800   0.008041         80.10  0.677503  24.150256   475.120361\n",
              "20        99.9875   0.006198         82.00  0.611381  24.194300   499.315558\n",
              "21        99.9975   0.004999         81.81  0.615755  24.186253   523.501914\n",
              "22       100.0000   0.004008         81.60  0.622091  24.169954   547.671987\n",
              "23        99.9900   0.004302         81.76  0.620312  24.152799   571.824895\n",
              "24       100.0000   0.003615         81.69  0.617631  24.144489   595.969493\n",
              "25       100.0000   0.003287         81.75  0.615135  24.226154   620.195747\n",
              "26       100.0000   0.002708         81.86  0.614319  24.152529   644.348380\n",
              "27       100.0000   0.002794         81.95  0.622269  24.113306   668.461797\n",
              "28       100.0000   0.002644         82.03  0.621709  24.089405   692.551306\n",
              "29       100.0000   0.002454         81.89  0.624302  24.141629   716.693051\n",
              "30       100.0000   0.002150         82.09  0.625112  24.162226   740.855379\n",
              "31       100.0000   0.002073         81.98  0.622740  24.127556   764.983053\n",
              "32       100.0000   0.002013         81.78  0.629925  24.149900   789.133072\n",
              "33       100.0000   0.001924         81.88  0.631050  24.123398   813.256585\n",
              "34       100.0000   0.001803         82.02  0.634264  24.168845   837.425535\n",
              "35       100.0000   0.001713         81.85  0.632225  24.159762   861.585407\n",
              "36       100.0000   0.001631         82.10  0.630084  24.107326   885.692845\n",
              "37       100.0000   0.001530         81.93  0.629895  24.129626   909.822589\n",
              "38       100.0000   0.001314         81.95  0.637243  24.090757   933.913454\n",
              "39       100.0000   0.001403         82.03  0.635496  24.105788   958.019346\n",
              "40       100.0000   0.001421         81.90  0.634566  24.104172   982.123624\n",
              "41       100.0000   0.001400         81.76  0.640637  24.112946  1006.236675\n",
              "42       100.0000   0.001284         81.92  0.646043  24.106823  1030.343614\n",
              "43       100.0000   0.001186         81.56  0.642000  24.122823  1054.466547\n",
              "44       100.0000   0.001230         81.82  0.641154  24.106619  1078.573263\n",
              "45       100.0000   0.001147         81.84  0.639139  24.098488  1102.671854\n",
              "46       100.0000   0.001135         81.96  0.639985  24.111058  1126.783018\n",
              "47       100.0000   0.001100         81.90  0.649073  24.107775  1150.890911\n",
              "48       100.0000   0.001067         81.78  0.645975  24.112036  1175.003069\n",
              "49       100.0000   0.001109         82.07  0.646042  24.122988  1199.126165\n",
              "50       100.0000   0.001016         81.73  0.646358  24.107152  1223.233426\n",
              "51       100.0000   0.000998         81.98  0.646003  24.097748  1247.331975\n",
              "52       100.0000   0.000961         81.77  0.652027  24.097799  1271.430642\n",
              "53       100.0000   0.001022         81.83  0.656071  24.078140  1295.509868\n",
              "54       100.0000   0.000924         81.94  0.644946  24.109716  1319.620442\n",
              "55       100.0000   0.000875         81.67  0.655117  24.092111  1343.713419\n",
              "56       100.0000   0.000857         81.84  0.649011  24.114468  1367.827991\n",
              "57       100.0000   0.000903         81.80  0.650456  24.103268  1391.932183\n",
              "58       100.0000   0.000860         81.70  0.658287  24.112809  1416.045914\n",
              "59       100.0000   0.000811         81.64  0.649958  24.122182  1440.168985\n",
              "60       100.0000   0.000845         81.71  0.649539  24.095495  1464.264612\n",
              "61       100.0000   0.000782         81.61  0.653397  24.119967  1488.385504\n",
              "62       100.0000   0.000790         81.61  0.652210  24.091106  1512.477528\n",
              "63       100.0000   0.000794         81.77  0.652456  24.115717  1536.594430"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a1a0520-377e-4860-848a-ff82754aa60e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TrainAccuracy</th>\n",
              "      <th>TrainLoss</th>\n",
              "      <th>TestAccuracy</th>\n",
              "      <th>TestLoss</th>\n",
              "      <th>EpochTime</th>\n",
              "      <th>TotalTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33.3675</td>\n",
              "      <td>1.971517</td>\n",
              "      <td>45.23</td>\n",
              "      <td>1.654450</td>\n",
              "      <td>22.663356</td>\n",
              "      <td>22.664520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50.5450</td>\n",
              "      <td>1.471566</td>\n",
              "      <td>56.42</td>\n",
              "      <td>1.262992</td>\n",
              "      <td>22.812373</td>\n",
              "      <td>45.477028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60.0850</td>\n",
              "      <td>1.178294</td>\n",
              "      <td>61.05</td>\n",
              "      <td>1.126179</td>\n",
              "      <td>23.044251</td>\n",
              "      <td>68.522231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>66.4175</td>\n",
              "      <td>0.983574</td>\n",
              "      <td>60.16</td>\n",
              "      <td>1.166780</td>\n",
              "      <td>23.236218</td>\n",
              "      <td>91.758553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>71.9750</td>\n",
              "      <td>0.828723</td>\n",
              "      <td>66.13</td>\n",
              "      <td>0.963742</td>\n",
              "      <td>23.405690</td>\n",
              "      <td>115.164344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>76.0225</td>\n",
              "      <td>0.712667</td>\n",
              "      <td>64.13</td>\n",
              "      <td>1.019803</td>\n",
              "      <td>23.564798</td>\n",
              "      <td>138.729245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>79.8350</td>\n",
              "      <td>0.603375</td>\n",
              "      <td>73.02</td>\n",
              "      <td>0.800099</td>\n",
              "      <td>23.654828</td>\n",
              "      <td>162.384912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>83.0150</td>\n",
              "      <td>0.507814</td>\n",
              "      <td>69.01</td>\n",
              "      <td>0.934989</td>\n",
              "      <td>23.766843</td>\n",
              "      <td>186.151879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>86.2400</td>\n",
              "      <td>0.421344</td>\n",
              "      <td>72.52</td>\n",
              "      <td>0.869184</td>\n",
              "      <td>23.856837</td>\n",
              "      <td>210.008825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>89.3150</td>\n",
              "      <td>0.336884</td>\n",
              "      <td>68.75</td>\n",
              "      <td>0.975435</td>\n",
              "      <td>23.928858</td>\n",
              "      <td>233.937788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>91.0975</td>\n",
              "      <td>0.280199</td>\n",
              "      <td>68.43</td>\n",
              "      <td>1.088275</td>\n",
              "      <td>23.969904</td>\n",
              "      <td>257.908579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>94.0325</td>\n",
              "      <td>0.204458</td>\n",
              "      <td>67.05</td>\n",
              "      <td>1.180282</td>\n",
              "      <td>24.005055</td>\n",
              "      <td>281.913751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>95.5450</td>\n",
              "      <td>0.159442</td>\n",
              "      <td>75.66</td>\n",
              "      <td>0.790307</td>\n",
              "      <td>24.066745</td>\n",
              "      <td>305.980646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>97.2275</td>\n",
              "      <td>0.109291</td>\n",
              "      <td>75.59</td>\n",
              "      <td>0.786821</td>\n",
              "      <td>24.164718</td>\n",
              "      <td>330.145481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>98.3575</td>\n",
              "      <td>0.073589</td>\n",
              "      <td>76.73</td>\n",
              "      <td>0.743234</td>\n",
              "      <td>24.215126</td>\n",
              "      <td>354.360719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>99.2700</td>\n",
              "      <td>0.045664</td>\n",
              "      <td>78.56</td>\n",
              "      <td>0.716787</td>\n",
              "      <td>24.144737</td>\n",
              "      <td>378.505563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>99.7750</td>\n",
              "      <td>0.024329</td>\n",
              "      <td>79.35</td>\n",
              "      <td>0.697416</td>\n",
              "      <td>24.141706</td>\n",
              "      <td>402.647385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>99.9125</td>\n",
              "      <td>0.015662</td>\n",
              "      <td>81.09</td>\n",
              "      <td>0.616303</td>\n",
              "      <td>24.190419</td>\n",
              "      <td>426.837930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>99.9550</td>\n",
              "      <td>0.010545</td>\n",
              "      <td>80.91</td>\n",
              "      <td>0.630850</td>\n",
              "      <td>24.131956</td>\n",
              "      <td>450.969998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>99.9800</td>\n",
              "      <td>0.008041</td>\n",
              "      <td>80.10</td>\n",
              "      <td>0.677503</td>\n",
              "      <td>24.150256</td>\n",
              "      <td>475.120361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>99.9875</td>\n",
              "      <td>0.006198</td>\n",
              "      <td>82.00</td>\n",
              "      <td>0.611381</td>\n",
              "      <td>24.194300</td>\n",
              "      <td>499.315558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>99.9975</td>\n",
              "      <td>0.004999</td>\n",
              "      <td>81.81</td>\n",
              "      <td>0.615755</td>\n",
              "      <td>24.186253</td>\n",
              "      <td>523.501914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.004008</td>\n",
              "      <td>81.60</td>\n",
              "      <td>0.622091</td>\n",
              "      <td>24.169954</td>\n",
              "      <td>547.671987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>99.9900</td>\n",
              "      <td>0.004302</td>\n",
              "      <td>81.76</td>\n",
              "      <td>0.620312</td>\n",
              "      <td>24.152799</td>\n",
              "      <td>571.824895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.003615</td>\n",
              "      <td>81.69</td>\n",
              "      <td>0.617631</td>\n",
              "      <td>24.144489</td>\n",
              "      <td>595.969493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.003287</td>\n",
              "      <td>81.75</td>\n",
              "      <td>0.615135</td>\n",
              "      <td>24.226154</td>\n",
              "      <td>620.195747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.002708</td>\n",
              "      <td>81.86</td>\n",
              "      <td>0.614319</td>\n",
              "      <td>24.152529</td>\n",
              "      <td>644.348380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.002794</td>\n",
              "      <td>81.95</td>\n",
              "      <td>0.622269</td>\n",
              "      <td>24.113306</td>\n",
              "      <td>668.461797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>82.03</td>\n",
              "      <td>0.621709</td>\n",
              "      <td>24.089405</td>\n",
              "      <td>692.551306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.002454</td>\n",
              "      <td>81.89</td>\n",
              "      <td>0.624302</td>\n",
              "      <td>24.141629</td>\n",
              "      <td>716.693051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.002150</td>\n",
              "      <td>82.09</td>\n",
              "      <td>0.625112</td>\n",
              "      <td>24.162226</td>\n",
              "      <td>740.855379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.002073</td>\n",
              "      <td>81.98</td>\n",
              "      <td>0.622740</td>\n",
              "      <td>24.127556</td>\n",
              "      <td>764.983053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.002013</td>\n",
              "      <td>81.78</td>\n",
              "      <td>0.629925</td>\n",
              "      <td>24.149900</td>\n",
              "      <td>789.133072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001924</td>\n",
              "      <td>81.88</td>\n",
              "      <td>0.631050</td>\n",
              "      <td>24.123398</td>\n",
              "      <td>813.256585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001803</td>\n",
              "      <td>82.02</td>\n",
              "      <td>0.634264</td>\n",
              "      <td>24.168845</td>\n",
              "      <td>837.425535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>81.85</td>\n",
              "      <td>0.632225</td>\n",
              "      <td>24.159762</td>\n",
              "      <td>861.585407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>82.10</td>\n",
              "      <td>0.630084</td>\n",
              "      <td>24.107326</td>\n",
              "      <td>885.692845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001530</td>\n",
              "      <td>81.93</td>\n",
              "      <td>0.629895</td>\n",
              "      <td>24.129626</td>\n",
              "      <td>909.822589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>81.95</td>\n",
              "      <td>0.637243</td>\n",
              "      <td>24.090757</td>\n",
              "      <td>933.913454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001403</td>\n",
              "      <td>82.03</td>\n",
              "      <td>0.635496</td>\n",
              "      <td>24.105788</td>\n",
              "      <td>958.019346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001421</td>\n",
              "      <td>81.90</td>\n",
              "      <td>0.634566</td>\n",
              "      <td>24.104172</td>\n",
              "      <td>982.123624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>81.76</td>\n",
              "      <td>0.640637</td>\n",
              "      <td>24.112946</td>\n",
              "      <td>1006.236675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>81.92</td>\n",
              "      <td>0.646043</td>\n",
              "      <td>24.106823</td>\n",
              "      <td>1030.343614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>81.56</td>\n",
              "      <td>0.642000</td>\n",
              "      <td>24.122823</td>\n",
              "      <td>1054.466547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>81.82</td>\n",
              "      <td>0.641154</td>\n",
              "      <td>24.106619</td>\n",
              "      <td>1078.573263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>81.84</td>\n",
              "      <td>0.639139</td>\n",
              "      <td>24.098488</td>\n",
              "      <td>1102.671854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>81.96</td>\n",
              "      <td>0.639985</td>\n",
              "      <td>24.111058</td>\n",
              "      <td>1126.783018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>81.90</td>\n",
              "      <td>0.649073</td>\n",
              "      <td>24.107775</td>\n",
              "      <td>1150.890911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>81.78</td>\n",
              "      <td>0.645975</td>\n",
              "      <td>24.112036</td>\n",
              "      <td>1175.003069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>82.07</td>\n",
              "      <td>0.646042</td>\n",
              "      <td>24.122988</td>\n",
              "      <td>1199.126165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>81.73</td>\n",
              "      <td>0.646358</td>\n",
              "      <td>24.107152</td>\n",
              "      <td>1223.233426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000998</td>\n",
              "      <td>81.98</td>\n",
              "      <td>0.646003</td>\n",
              "      <td>24.097748</td>\n",
              "      <td>1247.331975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>81.77</td>\n",
              "      <td>0.652027</td>\n",
              "      <td>24.097799</td>\n",
              "      <td>1271.430642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>81.83</td>\n",
              "      <td>0.656071</td>\n",
              "      <td>24.078140</td>\n",
              "      <td>1295.509868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>81.94</td>\n",
              "      <td>0.644946</td>\n",
              "      <td>24.109716</td>\n",
              "      <td>1319.620442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>81.67</td>\n",
              "      <td>0.655117</td>\n",
              "      <td>24.092111</td>\n",
              "      <td>1343.713419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>81.84</td>\n",
              "      <td>0.649011</td>\n",
              "      <td>24.114468</td>\n",
              "      <td>1367.827991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>81.80</td>\n",
              "      <td>0.650456</td>\n",
              "      <td>24.103268</td>\n",
              "      <td>1391.932183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000860</td>\n",
              "      <td>81.70</td>\n",
              "      <td>0.658287</td>\n",
              "      <td>24.112809</td>\n",
              "      <td>1416.045914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>81.64</td>\n",
              "      <td>0.649958</td>\n",
              "      <td>24.122182</td>\n",
              "      <td>1440.168985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>81.71</td>\n",
              "      <td>0.649539</td>\n",
              "      <td>24.095495</td>\n",
              "      <td>1464.264612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>81.61</td>\n",
              "      <td>0.653397</td>\n",
              "      <td>24.119967</td>\n",
              "      <td>1488.385504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000790</td>\n",
              "      <td>81.61</td>\n",
              "      <td>0.652210</td>\n",
              "      <td>24.091106</td>\n",
              "      <td>1512.477528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>100.0000</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>81.77</td>\n",
              "      <td>0.652456</td>\n",
              "      <td>24.115717</td>\n",
              "      <td>1536.594430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a1a0520-377e-4860-848a-ff82754aa60e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9a1a0520-377e-4860-848a-ff82754aa60e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9a1a0520-377e-4860-848a-ff82754aa60e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-85dd6704-ab78-4930-82c5-fadfe3c2872f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85dd6704-ab78-4930-82c5-fadfe3c2872f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-85dd6704-ab78-4930-82c5-fadfe3c2872f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "df_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K-r5RE5kOft"
      },
      "source": [
        "## Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHGK9EWrjvvx",
        "outputId": "bc29036c-9f96-453b-d4c0-4388aa16a1eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest accuracy reached: 82.1%\n"
          ]
        }
      ],
      "source": [
        "max_accuracy = df_res['TestAccuracy'].max()\n",
        "print(f\"Highest accuracy reached: {max_accuracy}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}